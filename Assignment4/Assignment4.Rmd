---
title: "Assignment 4 R Notebook"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
editor_options:
  markdown:
    wrap: 72
---

# Assignment 4 - Predictive Modeling

1.  [**Load into R the expression data and matching metadata that you
    processed in Assignment 2.**]{.underline}

Please insert your data files in the data directory such that the single
folder from refine.bio containing the metadata TSV and main TSV file are
present in the immediate subdirectory. This can be downloaded from
google drive,
[here!](https://drive.google.com/file/d/1aOnupOgIn-b7rSoGdRflNBvpfQPcR5v5/view?usp=sharing "Download folder to be unzipped as subdirectory of Data!")

This next step loads data and annotation libraries per the instructions,
but does not attempt to transform our Gene column to the Entrez IDs as
this was unnecessary for downstream processing.

```{r include=FALSE}
# Mount requisite libraries, install clustering lib
library(dplyr)
library(tidyverse)
library(readr)
library(factoextra)
library(caret)
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(data.table)
remotes::install_github("mlr-org/mlr3extralearners@*release")
library(mlr3verse)
library(mlr3tuning)
library(data.table)
library(ggplot2)
library(ggalluvial)
library(magrittr)
library(mlr3extralearners)
library(pheatmap)
library(ranger)

# Create the data folder if it doesn't exist
if (!dir.exists("data")) {
  dir.create("data")
}
# Define the file path to the plots directory
plots_dir <- "plots"
# Create the plots folder if it doesn't exist
if (!dir.exists(plots_dir)) {
  dir.create(plots_dir)
}
# Define the file path to the results directory
results_dir <- "results"
# Create the results folder if it doesn't exist
if (!dir.exists(results_dir)) {
  dir.create(results_dir)
}
# Define the file path to the data directory
data_dir <- file.path("data", "SRP075377")
# Declare the file path to the gene expression matrix file inside directory saved as `data_dir`
data_file <- file.path(data_dir, "SRP075377.tsv")
# Declare the file path to the metadata file inside the directory saved as `data_dir`
metadata_file <- file.path(data_dir, "metadata_SRP075377.tsv")
# Attach library for pipe (%>%)
```

```{r}
# Check that files exist and are usable.
file.exists(data_file)
file.exists(metadata_file)
```

```{r include=FALSE}
# Read in data and metadata TSV file and make Gene column into row names
metadata <- readr::read_tsv(metadata_file)
new_expression_df <- readr::read_tsv(data_file) %>%
  tibble::column_to_rownames("Gene")

metadata <- metadata %>%
  dplyr::mutate(diabetes = dplyr::case_when(
    stringr::str_detect(refinebio_subject, "non t2d") ~ "reference",
    stringr::str_detect(refinebio_subject, "t2d") ~ "diabetic",
  ))

#Remove ambiguously-labeled samples from metadata
culledMeta <- metadata[!(metadata$refinebio_subject=="pancreatic islets"),]

discardColumns <- metadata[(metadata$refinebio_subject=="pancreatic islets"),]
discardColumns = as.vector(discardColumns$refinebio_accession_code)
length(discardColumns)
#Preserve only columns in expression_df that match one of the accession ids
culled_expression_df = new_expression_df[,!(names(new_expression_df) %in% discardColumns)]

#expression_df <- as.data.frame(t(culled_expression_df))
expression_df <- culled_expression_df
```

```{r}
#
referenceGroup <- subset(culledMeta, select = c("refinebio_accession_code", "diabetes"))
referenceGroup <- referenceGroup %>%
  dplyr::mutate(diabetes = dplyr::case_when(
    stringr::str_detect(diabetes, "reference") ~ 0,
    stringr::str_detect(diabetes, "diabetic") ~ 1,
  ))

reference_group <- referenceGroup$diabetes
```

2. Supervised Analysis
  a. Subset your data to the 5,000 most variable genes
  b. Using that subset of the data, select and run an algorithm from this list:
    i. Support vector machine
    ii. Logistic regression
    iii. Random forest
    iv. K nearest neighbors
    v. Naïve Bayes
  c. I recommend using either TidyModels or the mlr3 package. Tutorials:
    i. https://mlr3.mlr-org.com/
    ii. https://seandavi.github.io/ITR/machine_learning_mlr3.html
    iii. https://www.tidymodels.org/start/
  d. Have the algorithms predict the two groups from assignment 1 (e.g. tumor vs normal)
  e. Each student in your team should run a different supervised analysis method (e.g., if      there are 4 students on the team, there should be results for 4 predictive methods in      your assignment writeup).
  f. Extract the gene signatures from the model.
    i. How many genes are in each of the predictive method signatures?
    ii. How much overlap is there between the signatures?
  g. Rerun each predictive method using different numbers of genes. Try 10, 100, 1000, and      10000 genes.
    i. How did the number of genes affect the results?
    ii. Are the same genes being included in the models? How much overlap is there between         the signatures?
    iii. What is the model performance (AUC) for each of the different versions of the              model? Does it increase or decrease as the number of genes included in the model           changes?

# 2A: Subset your data to the 5,000 most variable genes

```{r include=FALSE}
# Calculate row variance
row_variances <- apply(expression_df, 1, mad)
expression_df$row_variance <- row_variances
# Sort by row variance
expression_df <- expression_df[order(row_variances, decreasing = TRUE), ]

# Remove the row variance column
expression_df <- select(expression_df, -row_variance)

top_5000 <- head(expression_df, 5000)
top_5000 <- as.data.frame(t(top_5000))

top_10 <- head(expression_df, 10)
top_100 <- head(expression_df, 100)
top_1000 <- head(expression_df, 1000)
top_10000 <- head(expression_df, 10000)
top_10 <- as.data.frame(t(top_10))
top_100 <- as.data.frame(t(top_100))
top_1000 <- as.data.frame(t(top_1000))
top_10000 <- as.data.frame(t(top_10000))
```

# 2B: Using that subset of the data, select and run an algorithm from this list:
    i. Support vector machine
    ii. Logistic regression
    iii. Random forest
    iv. K nearest neighbors
    v. Naïve Bayes
    
    
# Random Forest Supervised Learning Algorithm

# Random Forest Algorithm Top 5000 Most Variable Genes
# ```{r}
# # Split the data into training and testing sets
# train_indices <- createDataPartition(culledMeta$diabetes, p = 0.7, list = FALSE)
# train_data <- top_5000[train_indices, ]
# test_data <- top_5000[-train_indices, ]
# train_labels <- culledMeta$diabetes[train_indices]
#
# # Train a Random Forest model
# rf_model <- train(
#   x = train_data,
#   y = train_labels,
#   method = "rf",
#   trControl = trainControl(method = "cv")
# )
#
# # Make predictions on the test data
# predictions_Top_5000 <- predict(rf_model, newdata = test_data)
#
#
# # Extract the gene signatures from the Random Forest model.
# # Extract variable importance
# variable_importance <- varImp(rf_model)
#
# # Get the top important genes
# gene_Signatures_Top_5000 <- rownames(variable_importance$importance)
# ```

# Random Forest Algorithm Top 10 Most Variable Genes
```{r}
# Split the data into training and testing sets
train_indices <- createDataPartition(culledMeta$diabetes, p = 0.7, list = FALSE)
train_data <- top_10[train_indices, ]
test_data <- top_10[-train_indices, ]
train_labels <- culledMeta$diabetes[train_indices]
 
# Train a Random Forest model
rf_model <- train(
  x = train_data,
  y = train_labels,
  method = "rf",
  trControl = trainControl(method = "cv")
)
 
# Make predictions on the test data
predictions_Top_10 <- predict(rf_model, newdata = test_data)

# Extract the gene signatures from the Random Forest model.
# Extract variable importance
variable_importance <- varImp(rf_model)

# Get the top important genes
gene_Signatures_Top_10 <- rownames(variable_importance$importance)
```

# Random Forest Algorithm Top 100 Most Variable Genes
```{r}
# Split the data into training and testing sets
train_indices <- createDataPartition(culledMeta$diabetes, p = 0.7, list = FALSE)
train_data <- top_100[train_indices, ]
test_data <- top_100[-train_indices, ]
train_labels <- culledMeta$diabetes[train_indices]
 
# Train a Random Forest model
rf_model <- train(
  x = train_data,
  y = train_labels,
  method = "rf",
  trControl = trainControl(method = "cv")
)
 
# Make predictions on the test data
predictions_Top_100 <- predict(rf_model, newdata = test_data)


# Extract the gene signatures from the Random Forest model.
# Extract variable importance
variable_importance <- varImp(rf_model)

# Get the top important genes
gene_Signatures_Top_100 <- rownames(variable_importance$importance)
```

# Random Forest Algorithm Top 1000 Most Variable Genes
```{r}
# Split the data into training and testing sets
train_indices <- createDataPartition(culledMeta$diabetes, p = 0.7, list = FALSE)
train_data <- top_1000[train_indices, ]
test_data <- top_1000[-train_indices, ]
train_labels <- culledMeta$diabetes[train_indices]
 
# Train a Random Forest model
rf_model <- train(
  x = train_data,
  y = train_labels,
  method = "rf",
  trControl = trainControl(method = "cv")
)
 
# Make predictions on the test data
predictions_Top_1000 <- predict(rf_model, newdata = test_data)


# Extract the gene signatures from the Random Forest model.
# Extract variable importance
variable_importance <- varImp(rf_model)

# Get the top important genes
gene_Signatures_Top_1000 <- rownames(variable_importance$importance)
```

# Random Forest Algorithm Top 10000 Most Variable Genes
# ```{r}
# # Split the data into training and testing sets
# train_indices <- createDataPartition(culledMeta$diabetes, p = 0.7, list = FALSE)
# train_data <- top_10000[train_indices, ]
# test_data <- top_10000[-train_indices, ]
# train_labels <- culledMeta$diabetes[train_indices]
#  
# # Train a Random Forest model
# rf_model <- train(
#   x = train_data,
#   y = train_labels,
#   method = "rf",
#   trControl = trainControl(method = "cv")
# )
#  
# # Make predictions on the test data
# predictions_Top_10000 <- predict(rf_model, newdata = test_data)
# 
# 
# # Extract the gene signatures from the Random Forest model.
# # Extract variable importance
# variable_importance <- varImp(rf_model)
# 
# # Get the top important genes
# gene_Signatures_Top_10000 <- rownames(variable_importance$importance)
# ```

```{r}
#Rip out those pesky rownames, join on that column, then drop the column
top_10$refinebio_accession_code <- rownames(top_10) 
top_10_labeled <- merge(top_10, referenceGroup, by = "refinebio_accession_code")
top_10 <- select(top_10, -refinebio_accession_code)
rownames(top_10_labeled) <- top_10_labeled$refinebio_accession_code
top_10_labeled <- select(top_10_labeled, -refinebio_accession_code)

#Turn binary column into a factor
top_10_labeled$diabetes <- factor(top_10_labeled$diabetes, levels = c(0, 1), labels = c("Non-Diabetic", "Diabetic"))

task <- TaskClassif$new(id = "gene_exp", backend = top_10_labeled, target = "diabetes")
split = partition(task, ratio = 0.7)
learner <- lrn("classif.svm", kernel = "linear")
svm_model <- learner$train(task, split$train)
predictions <- learner$predict(task, split$test)
model <- svm_model$model

# Get genes in top weight percentile by percent below:
gene_weights <- t(model$SV) %*% model$coefs
threshold <- quantile(abs(gene_weights), 0.50)

# Get the genes by column index
signature_indices <- which(abs(as.numeric(gene_weights)) > threshold)
signature_genes_SVM_10_50 <- colnames(top_10_labeled)[signature_indices]

# Get some metrics
cm = predictions$confusion
auc = predictions$score(msr("classif.acc"))
TPR = cm[2,2] / (cm[2,2] + cm[2,1])  # Sensitivity or Recall: TP / (TP + FN)
FPR = cm[1,2] / (cm[1,2] + cm[1,1])  # False Positive Rate: FP / (FP + TN)

# Spit it all out
print(auc)
print(cm)
cat("True Positive Rate:", TPR, "\n")
cat("False Positive Rate:", FPR, "\n")
```

```{r}
#Rip out those pesky rownames, join on that column, then drop the column
top_100$refinebio_accession_code <- rownames(top_100) 
top_100_labeled <- merge(top_100, referenceGroup, by = "refinebio_accession_code")
top_100 <- select(top_100, -refinebio_accession_code)
rownames(top_100_labeled) <- top_100_labeled$refinebio_accession_code
top_100_labeled <- select(top_100_labeled, -refinebio_accession_code)

#Turn binary column into a factor
top_100_labeled$diabetes <- factor(top_100_labeled$diabetes, levels = c(0, 1), labels = c("Non-Diabetic", "Diabetic"))

task <- TaskClassif$new(id = "gene_exp", backend = top_100_labeled, target = "diabetes")
split = partition(task, ratio = 0.7)
learner <- lrn("classif.svm", kernel = "linear")
svm_model <- learner$train(task, split$train)
predictions <- learner$predict(task, split$test)
model <- svm_model$model

# Get genes in top weight percentile by percent below:
gene_weights <- t(model$SV) %*% model$coefs
threshold <- quantile(abs(gene_weights), 0.90)

# Get the genes by column index
signature_indices <- which(abs(as.numeric(gene_weights)) > threshold)
signature_genes_SVM_100_90 <- colnames(top_100_labeled)[signature_indices]

# Get some metrics
cm = predictions$confusion
auc = predictions$score(msr("classif.acc"))
TPR = cm[2,2] / (cm[2,2] + cm[2,1])  # Sensitivity or Recall: TP / (TP + FN)
FPR = cm[1,2] / (cm[1,2] + cm[1,1])  # False Positive Rate: FP / (FP + TN)

# Spit it all out
print(auc)
print(cm)
cat("True Positive Rate:", TPR, "\n")
cat("False Positive Rate:", FPR, "\n")
```

```{r}
#Rip out those pesky rownames, join on that column, then drop the column
top_1000$refinebio_accession_code <- rownames(top_1000) 
top_1000_labeled <- merge(top_1000, referenceGroup, by = "refinebio_accession_code")
top_1000 <- select(top_1000, -refinebio_accession_code)
rownames(top_1000_labeled) <- top_1000_labeled$refinebio_accession_code
top_1000_labeled <- select(top_1000_labeled, -refinebio_accession_code)

#Turn binary column into a factor
top_1000_labeled$diabetes <- factor(top_1000_labeled$diabetes, levels = c(0, 1), labels = c("Non-Diabetic", "Diabetic"))

task <- TaskClassif$new(id = "gene_exp", backend = top_1000_labeled, target = "diabetes")
split = partition(task, ratio = 0.7)
learner <- lrn("classif.svm", kernel = "linear")
svm_model <- learner$train(task, split$train)
predictions <- learner$predict(task, split$test)
model <- svm_model$model

# Get genes in top weight percentile by percent below:
gene_weights <- t(model$SV) %*% model$coefs
threshold <- quantile(abs(gene_weights), 0.99)

# Get the genes by column index
signature_indices <- which(abs(as.numeric(gene_weights)) > threshold)
signature_genes_SVM_1000_99 <- colnames(top_1000_labeled)[signature_indices]

# Get some metrics
cm = predictions$confusion
auc = predictions$score(msr("classif.acc"))
TPR = cm[2,2] / (cm[2,2] + cm[2,1])  # Sensitivity or Recall: TP / (TP + FN)
FPR = cm[1,2] / (cm[1,2] + cm[1,1])  # False Positive Rate: FP / (FP + TN)

# Spit it all out
print(auc)
print(cm)
cat("True Positive Rate:", TPR, "\n")
cat("False Positive Rate:", FPR, "\n")
```

```{r}
#Rip out those pesky rownames, join on that column, then drop the column
top_5000$refinebio_accession_code <- rownames(top_5000) 
top_5000_labeled <- merge(top_5000, referenceGroup, by = "refinebio_accession_code")
top_5000 <- select(top_5000, -refinebio_accession_code)
rownames(top_5000_labeled) <- top_5000_labeled$refinebio_accession_code
top_5000_labeled <- select(top_5000_labeled, -refinebio_accession_code)

#Turn binary column into a factor
top_5000_labeled$diabetes <- factor(top_5000_labeled$diabetes, levels = c(0, 1), labels = c("Non-Diabetic", "Diabetic"))

task <- TaskClassif$new(id = "gene_exp", backend = top_5000_labeled, target = "diabetes")
split = partition(task, ratio = 0.7)
learner <- lrn("classif.svm", kernel = "linear")
svm_model <- learner$train(task, split$train)
predictions <- learner$predict(task, split$test)
model <- svm_model$model

# Get genes in top weight percentile by percent below:
gene_weights <- t(model$SV) %*% model$coefs
threshold <- quantile(abs(gene_weights), 0.999)

# Get the genes by column index
signature_indices <- which(abs(as.numeric(gene_weights)) > threshold)
signature_genes_SVM_5000_999 <- colnames(top_5000_labeled)[signature_indices]

# Get some metrics
cm = predictions$confusion
auc = predictions$score(msr("classif.acc"))
TPR = cm[2,2] / (cm[2,2] + cm[2,1])  # Sensitivity or Recall: TP / (TP + FN)
FPR = cm[1,2] / (cm[1,2] + cm[1,1])  # False Positive Rate: FP / (FP + TN)

# Spit it all out
print(auc)
print(cm)
cat("True Positive Rate:", TPR, "\n")
cat("False Positive Rate:", FPR, "\n")
```

```{r}
#Rip out those pesky rownames, join on that column, then drop the column
top_10000$refinebio_accession_code <- rownames(top_10000) 
top_10000_labeled <- merge(top_10000, referenceGroup, by = "refinebio_accession_code")
top_10000 <- select(top_10000, -refinebio_accession_code)
rownames(top_10000_labeled) <- top_10000_labeled$refinebio_accession_code
top_10000_labeled <- select(top_10000_labeled, -refinebio_accession_code)

#Turn binary column into a factor
top_10000_labeled$diabetes <- factor(top_10000_labeled$diabetes, levels = c(0, 1), labels = c("Non-Diabetic", "Diabetic"))

task <- TaskClassif$new(id = "gene_exp", backend = top_10000_labeled, target = "diabetes")
split = partition(task, ratio = 0.7)
learner <- lrn("classif.svm", kernel = "linear")
svm_model <- learner$train(task, split$train)
predictions <- learner$predict(task, split$test)
model <- svm_model$model

# Get genes in top weight percentile by percent below:
gene_weights <- t(model$SV) %*% model$coefs
threshold <- quantile(abs(gene_weights), 0.999)

# Get the genes by column index
signature_indices <- which(abs(as.numeric(gene_weights)) > threshold)
signature_genes_SVM_10000_999 <- colnames(top_10000_labeled)[signature_indices]

# Get some metrics
cm = predictions$confusion
auc = predictions$score(msr("classif.acc"))
TPR = cm[2,2] / (cm[2,2] + cm[2,1])  # Sensitivity or Recall: TP / (TP + FN)
FPR = cm[1,2] / (cm[1,2] + cm[1,1])  # False Positive Rate: FP / (FP + TN)

# Spit it all out
print(auc)
print(cm)
cat("True Positive Rate:", TPR, "\n")
cat("False Positive Rate:", FPR, "\n")
```
Model performance degrades further at 10,000 genes. In order to attempt to correct this I explored various kernels and hyperparameters including RBF, Polynomial, and Sigmoid. None of these improved performance although polynomial with a high coefficient got better results than the others. What we're witnessing here is the "curse of dimensionality" to SVM. It is overfitting to the noise produced by extending the feature vector to include less relevant genes. I also attempted to improve results, particularly the 0 TN FP rate (divide by zero error in AUC calculation) by setting the classifier's class weights to the inverse of their frequency as was recommended in several websites. Unfortunately this did not improve the outcome, and SVM remains victim to overfitting high-dimensional noise.
