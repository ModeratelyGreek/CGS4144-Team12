---
title: "Assignment 3 R Notebook"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
editor_options:
  markdown:
    wrap: 72
---

## Welcome to Assignment 3, Data Exploration

# Instructions:

Below we've transcribed the assignment instructions with corresponding
code blocks beneath each.

[ðŸš¨**Please note,**]{.underline}

-   With the exception of headers and the above, writing in the normal
    font is ours, and [**bold/underlined writing is copy-pasted from the
    instructions.**]{.underline}

-   Code blocks were directly excerpted, adapted, and reused from
    previously provided instructions / guidance. We do not believe this
    to be plagiarism and hope that in declaring it up front we
    demonstrate our honest intent. The sources used are found in the
    Assignment 2 instructions (which are transcribed fully in the
    Assignment 2 folder in our repo).

-   Task 5: Annotating tables, has been performed inline, in a more
    traditional notebook manner. We do not reserve the annotations for
    step 5 and instead hope to meet the criteria by elaborating on all
    provided images and tables adjacently to each.

1.  [**Load into R the expression data and matching metadata that you
    processed in Assignment 2.**]{.underline}

Please insert your data files in the data directory such that the single
folder from refine.bio containing the metadata TSV and main TSV file are
present in the immediate subdirectory. This can be downloaded from
google drive,
[here!](https://drive.google.com/file/d/1aOnupOgIn-b7rSoGdRflNBvpfQPcR5v5/view?usp=sharing "Download folder to be unzipped as subdirectory of Data!")

This next step loads data and annotation libraries per the instructions,
but does not attempt to transform our Gene column to the Entrez IDs as
this was unnecessary for downstream processing.

```{r include=FALSE}
# Mount requisite libraries, install clustering lib
library(dplyr)
library(tidyverse)
library(readr)
library(factoextra)

if(!require("ggplot2"))
  install.packages("ggplot2")
if(!require("ggalluvial"))
  install.packages("ggalluvial")

# Create the data folder if it doesn't exist
if (!dir.exists("data")) {
  dir.create("data")
}

# Define the file path to the plots directory
plots_dir <- "plots"

# Create the plots folder if it doesn't exist
if (!dir.exists(plots_dir)) {
  dir.create(plots_dir)
}

# Define the file path to the results directory
results_dir <- "results"

# Create the results folder if it doesn't exist
if (!dir.exists(results_dir)) {
  dir.create(results_dir)
}

# Define the file path to the data directory
data_dir <- file.path("data", "SRP075377")

# Declare the file path to the gene expression matrix file inside directory saved as `data_dir`
data_file <- file.path(data_dir, "SRP075377.tsv")

# Declare the file path to the metadata file inside the directory saved as `data_dir`
metadata_file <- file.path(data_dir, "metadata_SRP075377.tsv")

# Attach library for pipe (%>%)
library(magrittr)
```

```{r}
# Check that files exist and are usable.
file.exists(data_file)
file.exists(metadata_file)
```

```{r include=FALSE}
# Read in data and metadata TSV file and make Gene column into row names
metadata <- readr::read_tsv(metadata_file)
expression_df <- readr::read_tsv(data_file) %>%
  tibble::column_to_rownames("Gene")
```

1.  [**Unsupervised Analysis**]{.underline}
    a.  [**Subset your data to the 5,000 most variable
        genes**]{.underline}
    b.  [**Using that subset of the data, select and run a clustering
        algorithm from this list:**]{.underline}
        1.  [**K-means**]{.underline}
        2.  [**Hierarchical clustering (hclust)**]{.underline}
        3.  [**ConsensusClusterPlus**]{.underline}
        4.  [**PAM Clustering**]{.underline}
        5.  [**Gaussian Mixture Models**]{.underline}
    c.  [**Each student in your team should run a different clustering
        method (e.g., if there are 4 students\
        on the team, there should be results for 4 clustering methods in
        your assignment writeup).**]{.underline}
    d.  [**How many clusters did each method find?**]{.underline}
        a.  [**If you ran a method that requires you to select the
            number of clusters (k), how did changing this value change
            the results? Compare cluster membership at each k to
            investigate this.**]{.underline}
        b.  [**If you ran a method that selects k for you, describe why
            it chose that k.**]{.underline}
    e.  [**Rerun each clustering method using different numbers of
        genes. Try 10, 100, 1000, and 10000 genes.**]{.underline}
        a.  [**How did the number of genes affect
            clustering?**]{.underline}
        b.  [**Create an alluvial diagram (Sankey plot) to visualize how
            the different clustering setups\
            changed cluster memberships for each sample.
            ([https://cran.r](https://cran.r-)project.org/web/packages/ggalluvial/vignettes/ggalluvial.html)**]{.underline}

# 1A: Subsetting to Most Variable Genes

```{r include=FALSE}
# Calculate row variance
# ðŸš¨ ConsensusClusterPlus suggests using MAD - median absolute deviation.
row_variances <- apply(expression_df, 1, mad)
expression_df$row_variance <- row_variances

# Sort by row variance
expression_df <- expression_df[order(row_variances, decreasing = TRUE), ]

# Remove the row variance columns
expression_df$row_variances <- NULL

# Keep quantities mentioned in part E
top_10 <- head(expression_df, 10)
top_100 <- head(expression_df, 100)
top_1000 <- head(expression_df, 1000)
top_5000 <- head(expression_df, 5000)
top_10000 <- head(expression_df, 10000)

top_10 <- as.data.frame(t(top_10))
top_100 <- as.data.frame(t(top_100))
top_1000 <- as.data.frame(t(top_1000))
top_5000 <- as.data.frame(t(top_5000))
top_10000 <- as.data.frame(t(top_10000))
```

# 1B:

## Rayyan - K-means

```{r INCLUDE=FALSE}
# compute ideal K for K-means clustering using the "elbow method" 

if(!require("purrr"))
  install.packages("purrr")

if(!require('factoextra'))
  install.packages('factoextra')

set.seed(123)
#Expect a delay, takes a bit
fviz_nbclust(top_5000, kmeans, k.max = 6, method = "wss")

# using the elbow method, the optimal number of clusters appears to be 2 (since this is around where the curve begins to flatten out most gently)
```

The elbow method visualized above reruns K-Means with different amounts
of clusters and visualizes how "Total within sum of square" decreases.
Whats that mean for us? The WSS metric explains proximity of points to
their centroid. Lower values mean tighter groupings. The elbow method
means we look for where the elbow in the curve occurs, picking a number
of clusters K that does not overfit the data, but also provides a
significant reduction in WSS compared to K-1 clusters. Here we choose 2
as optimal.

```{r}
#about 20 seconds
kmeans_result_5000_three <- kmeans(top_5000,3,iter.max = 10, nstart = 25)
kmeans_result_5000_four <- kmeans(top_5000,4,iter.max = 10, nstart = 25)
kmeans_result_5000_five <- kmeans(top_5000,5,iter.max = 10, nstart = 25)
```

```{r}
#About 30 seconds
fviz_cluster(kmeans_result_5000_four, data = top_5000, geom="point")
```

Above we can see how these 4 clusters look when displayed in a 2D graph
via PCA. This is a healthy "sanity-check" indicating our choice to use
four clusters (per elbow method) is indeed acceptable. The amount of
overlap seen in 3 and 1 is concerning but this is perhaps merely an
illusion of the 2D plane.

[**If you ran a method that requires you to select the number of
clusters (k), how did changing this value change the results? Compare
cluster membership at each k to investigate this.**]{.underline}

The K-means clustering method required me to select the number of
clusters. To determine the ideal "k", I utilized the "factoextra"
package to generate a graph showing the number of clusters versus the
total within the sum of square. Using the elbow method, I determined
that the optimal number of clusters was 2, and to test how changing the
value changed the results, ran the "kmeans" clustering command for a k
of 3,4, and 5 to determine how changing k changed the results.

Cluster membership at 3,4, and 5 was:

3 - 4358, 594, 48,

4 - 3878, 954, 147, 21

5 - 3806, 1003, 161, 20, 10

By assigning k of 3 as opposed to 5000, it achieved of a reduction in
sum of squares by 32.3%. At a k of 4, this rose greatly to 37.3% At a k
of 5, it rose only slightly to 39.9%. By looking at cluster membership
at k = 3,4,5, we can see that cluster membership does not change
drastically between 4 and 5. Between those 2 points, the top 3 largest
groups have largely the same numbers. Meanwhile, between a k of 3 and 4,
the top 3 largest groups have drastically different numbers. These data
together confirms the earlier elbow method, telling us that 4 is the
most ideal number of clusters.

```{r}
# rerun K-Means clustering method with different number of genes AND K values for the alluvial down the line
kmeans_result_10_three <- kmeans(top_10,3,iter.max = 10,nstart=25)
kmeans_result_10_four <- kmeans(top_10,4,iter.max = 10,nstart=25)
kmeans_result_10_five <- kmeans(top_10,5,iter.max = 10,nstart=25)
kmeans_result_100_three <- kmeans(top_100,3,iter.max = 10,nstart=25)
kmeans_result_100_four <- kmeans(top_100,4,iter.max = 10,nstart=25)
kmeans_result_100_five <- kmeans(top_100,5,iter.max = 10,nstart=25)
kmeans_result_1000_three <- kmeans(top_1000,3,iter.max = 10,nstart=25)
kmeans_result_1000_four <- kmeans(top_1000,4,iter.max = 10,nstart=25)
kmeans_result_1000_five <- kmeans(top_1000,5,iter.max = 10,nstart=25)
kmeans_result_10000_three <- kmeans(top_10000,3,iter.max = 10,nstart=25)
kmeans_result_10000_four <- kmeans(top_10000,4,iter.max = 10,nstart=25)
kmeans_result_10000_five <- kmeans(top_10000,5,iter.max = 10,nstart=25)

```

[**How did the number of genes affect clustering?**]{.underline}

Cluster Membership at 10, 100, 1000, and 10,000 (with a k of 4) was:

10 - 6, 2, 1, 1

100 - 80, 16, 2, 2

1000 - 810, 162, 18, 10

10,000 - 8296, 1517, 166, 21

Looking solely at cluster membership, it appears that each time the
number of genes increased by 10 times, membership in the 1,2nd, and 3rd
largest groups roughly followed the magnification as well. As the number
of genes increased, a reduction in sum of squares at a k of 4 was
calculated as 70.5%, 36.8%, 35.5,% and 39.1%.

```{r}
alluvialData <- data.frame(
  K3 = kmeans_result_10_three$cluster, 
  K4 = kmeans_result_10_four$cluster, 
  K5 = kmeans_result_10_five$cluster
)
ggplot(data = alluvialData, aes(axis1 = K3, axis2 = K4, axis3 = K5)) +
  geom_alluvium(aes(fill = K3)) +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme_minimal() +
  labs(title = "KMeans for 10 Samples at K=3,4,5")
```

The first of the alluvial plots here shows only 10 samples worth of
data. We can see how the samples in cluster 1 bifurcate early, while the
samples of cluster 2 at K=4 column bifurcate into groups 3 and 5 at K=5.
Nothing overwhelmingly insightful here, but lets see what happens with
more genes.

```{r}
alluvialData <- data.frame(
  K3 = kmeans_result_100_three$cluster, 
  K4 = kmeans_result_100_four$cluster, 
  K5 = kmeans_result_100_five$cluster
)
ggplot(data = alluvialData, aes(axis1 = K3, axis2 = K4, axis3 = K5)) +
  geom_alluvium(aes(fill = K3)) +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme_minimal() +
  labs(title = "KMeans for 100 Genes at K=3,4,5")

```

Things get more complicated with more genes. We can see the pattern of
bifurcations appears to be significantly different although of some
intrigue is the relatively robust group 3 chosen at K=3 which seems to
barely bifurcate at all (a small fraction goes to group 2). Perhaps this
group is the mostly tightly, whereas clearly the grey bands of group 1
from K3 are going many places indicating it was a poor discriminant and
likely of lesser significance.

```{r}
alluvialData <- data.frame(
  K3 = kmeans_result_1000_three$cluster, 
  K4 = kmeans_result_1000_four$cluster, 
  K5 = kmeans_result_1000_five$cluster
)
ggplot(data = alluvialData, aes(axis1 = K3, axis2 = K4, axis3 = K5)) +
  geom_alluvium(aes(fill = K3)) +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme_minimal() +
  labs(title = "KMeans for 1000 Genes at K=3,4,5")

```

Again the pattern changes dramatically. This is unsurprising given we
are adding more highly variable genes to the mix. Little more is to be
said.

```{r}
alluvialData <- data.frame(
  K3 = kmeans_result_10000_three$cluster, 
  K4 = kmeans_result_10000_four$cluster, 
  K5 = kmeans_result_10000_five$cluster
)
ggplot(data = alluvialData, aes(axis1 = K3, axis2 = K4, axis3 = K5)) +
  geom_alluvium(aes(fill = K3)) +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme_minimal() +
  labs(title = "KMeans for 10,000 Genes at K=3,4,5")

```

This last diagram is perhaps the most significant in examining groups
since it is based on the most data. Cluster 2(K3) is exceptional here
and bifurcates minimally indicating a high "goodness of clustering".

# Sahas - Hierarchical Clustering (hclust)

[Note, \~1 minute runtime on Macbook Air M2 (known for CPU), expect
delay.]{.underline}

```{r}
# 5000 most variable genes Hierarchical Clustering
distance <- dist(top_5000, method = "euclidean")
hcluster  <- hclust(distance, method = "complete")
# 1 cluster
hcluster_result_5000_two <- cutree(hcluster, k = 2)
# 2 clusters
hcluster_result_5000_three <- cutree(hcluster, k = 3)
# 3 clusters 
hcluster_result_5000_four <- cutree(hcluster, k = 4)

# Visualize dendrogram
h <- plot(hcluster, cex = 0.8, hang = -1)
#rect.hclust(hcluster, k = 2, border = 2:5)

# If you ran a method that requires you to select the number of clusters (k), how did changing # this value change the results? Compare cluster membership at each k to investigate this.

# Visualize clusters
fviz_cluster(list(data = top_5000, cluster = hcluster_result_5000_two), geom = "point")
fviz_cluster(list(data = top_5000, cluster = hcluster_result_5000_three), geom = "point")
fviz_cluster(list(data = top_5000, cluster = hcluster_result_5000_four), geom = "point")
```

This set of graphs is quite interesting. The two cluster graph amusingly
displays what appears to be a single cluster, likely another folly of
the 2D world. The three cluster graph shows how the larger visible
cluster has bifurcated (this time quite literally given the nature of
hierarchical clustering). The four cluster graph further divides these
two larger clusters, though it is unclear from which the points of
cluster 1 originate, but the extremely tight cluster (now cluster 4)
remains centered and nearly invisible.

Expect further delay (\~1 mins?)

```{r}
# HClust Top 10 fails to work here, as there is presumably not enough data? The origin of the issue is unclear but the dendrogram is deemed invalid.

# 10 most variable genes Hierarchical Clustering
#distance <- dist(top_10, method = "euclidean")
#hclusterTop10  <- hclust(distance, method = "complete")
#plot(hclusterTop10, cex = 0.8, hang = -1, labels = TRUE)
#hcluster_result_10_two <- cutree(hclusterTop10, k = 2)

# 100 most variable genes Hierarchical Clustering
distance <- dist(top_100, method = "euclidean")
hclusterTop100  <- hclust(distance, method = "complete")
plot(hclusterTop100, cex = 0.8, hang = -1, labels = FALSE)
hcluster_result_100_two <- cutree(hclusterTop100, k = 2)

# 1000 most variable genes Hierarchical Clustering
distance <- dist(top_1000, method = "euclidean")
hclusterTop1000  <- hclust(distance, method = "complete")
plot(hclusterTop1000, cex = 0.8, hang = -1, labels = FALSE)
hcluster_result_1000_two <- cutree(hclusterTop1000, k = 2)

# 10000 most variable genes Hierarchical Clustering
distance <- dist(top_10000, method = "euclidean")
hclusterTop10000  <- hclust(distance, method = "complete")
plot(hclusterTop10000, cex = 0.8, hang = -1, labels = FALSE)
hcluster_result_10000_two <- cutree(hclusterTop10000, k = 2)
```

The dendrograms above help to illustrate the tree being produced by
running variations of hierarchical clustering. Of interest is the very
large offshoot of the majority of the genes segregated away from smaller
groups in the 10,000 gene dendrogram. This will have consequences on the
alluvial visuals below, and raises questions about logarithmically
scaling the width of the bands alluvial diagrams produce, rather than
linearly.

Due to the nature of hierarchical clustering making the alluvial
diagrams exceptionally disinteresting, we've opted to highlight only
alluvials at 10 genes and 10,000 genes but with more divisions (you're
more or less looking at a horizontal dendrogram...)

```{r}
hcluster_result_10_two <- cutree(hclusterTop10, k = 2)
hcluster_result_10_three <- cutree(hclusterTop10, k = 3)
hcluster_result_10_four <- cutree(hclusterTop10, k = 4)
hcluster_result_10_five <- cutree(hclusterTop10, k = 5)
hcluster_result_10000_two <- cutree(hclusterTop10000, k = 2)
hcluster_result_10000_three <- cutree(hclusterTop10000, k = 3)
hcluster_result_10000_four <- cutree(hclusterTop10000, k = 4)
hcluster_result_10000_five <- cutree(hclusterTop10000, k = 5)

alluvial_data <- data.frame(
  K2 = hcluster_result_10_two,
  K3 = hcluster_result_10_three,
  K4 = hcluster_result_10_four,
  K5 = hcluster_result_10_five
)

ggplot(data = alluvial_data, aes(axis1 = K2, axis2 = K3, axis3 = K4, axis4 = K5)) +
  geom_alluvium(aes(fill = K2)) +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme_minimal() +
  labs(title = "Hierarchical Clustering across different values of K at 10 genes")

alluvial_data <- data.frame(
  K2 = hcluster_result_10000_two,
  K3 = hcluster_result_10000_three,
  K4 = hcluster_result_10000_four,
  K5 = hcluster_result_10000_five
)

ggplot(data = alluvial_data, aes(axis1 = K2, axis2 = K3, axis3 = K4, axis4 = K5)) +
  geom_alluvium(aes(fill = K2)) +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme_minimal() +
  labs(title = "Hierarchical Clustering across different values of K at 10000 genes")
```

There are two diagrams here, the first showing almost nothing of
interest because as per the nature of hierarchical clustering, groups
are perfectly divided and do not allow samples to cross over to other
branches halfway down the tree. Thus from K2\--\>3 to 3, 2 bifurcates to
2/3, from K3\--\>4, 1 bifurcates to 1 and 2, and so on. Very simple. As
for the second alluvial...

You're probably thinking "that doesn't look right". Au contraire. If we
look at the dendrogram for 10,000 genes we see how the ensuing splits in
the hierarchy take place on the right side with one giant arm reaching
to the left This is the "cluster 2" which is renamed to cluster 4 on the
final column by chance, but represents this large undivided branch of
the tree that is only subdivided later in the hierarchy.

# Michael - PAM Clustering

```{r}
library(cluster)

#Center data around median gene expressions in each row, without touching OG data. This came from the ConsensusClusterPlus guide, though "Scale" which the internet recommended for use with PAM would accomplish the same thing just with means instead of medians.
top_10m = sweep(top_10,1, apply(top_10,1,median,na.rm=T))
top_100m = sweep(top_100,1, apply(top_100,1,median,na.rm=T))
top_1000m = sweep(top_1000,1, apply(top_1000,1,median,na.rm=T))
top_5000m = sweep(top_5000,1, apply(top_5000,1,median,na.rm=T))
top_10000m = sweep(top_10000,1, apply(top_10000,1,median,na.rm=T))

#Takes quite a while at 10,000
k <- 3
pam_result_10_three <- pam(top_10m, k = k)
pam_result_100_three <- pam(top_100m, k = k)
pam_result_1000_three <- pam(top_1000m, k = k)
pam_result_5000_three <- pam(top_5000m, k = k)
pam_result_10000_three <- pam(top_10000m, k = k)
```

ðŸš¨The instructions linked to PAM suggested the use of a dissimilarity
matrix but I couldnt get fviz and other operations downstream to work
with it, so im just doing PAM the classic way (it generates the
dissimilarity matrix). I suspect this is why the run time is so
crippling. Please consider enjoying some popcorn or perhaps painting the
nearest surface before hitting run on this next cell, so that you may
enjoy some semblance of stimulation. Your RAM usage graph may also be
amusing to watch.

```{r}
#Let's compare K values:

cluster_counts <- table(pam_result_5000_three$clustering)
cluster_counts

pam_result_5000_four <- pam(top_5000m, k = 4)
cluster_counts <- table(pam_result_5000_four$clustering)
cluster_counts

pam_result_5000_five <- pam(top_5000m, k = 5)
cluster_counts <- table(pam_result_5000_five$clustering)
cluster_counts

pam_result_5000_six <- pam(top_5000m, k = 6)
cluster_counts <- table(pam_result_5000_six$clustering)
cluster_counts
```

```{r}
fviz_cluster(pam_result_5000_three, geom = "point")
fviz_cluster(pam_result_5000_four,  geom = "point")
fviz_cluster(pam_result_5000_five, geom = "point")
fviz_cluster(pam_result_5000_six, geom = "point")
```

We can see how membership changes above, at least in the two chosen
dimensions for the visualization (since we're operating on
5000-dimensional data). Indeed it seems that more clusters reduce the
overlap, but the extent to which they reduce the overlap is better
demonstrated through the sillhouette method shown below, because,
amusingly, the new clusters introduced in the 2D field above show
literally 0 reduction in overlap. 4 of the generated clusters overlap
almost perfectly with eachother, implying the PCA has failed to
adequately visualize these groups.

ðŸš¨[Note, takes \~4 minutes on macbook air M2.]{.underline}

```{r}
#Takes around 4 minutes on a macbook air M2
library(factoextra)
fviz_nbclust(top_5000m, pam, k.max = 6, verbose = TRUE, print.summary = TRUE, method = "silhouette")

#should identify 2 as optimal K value, which is a function of sillhouette score
```

The sillhouette method used above is looking at all the points resulting
from different K values of PAM, giving them a score (based on their
proximity to their assigned cluster and distance to other clusters), and
then averaging the score accross all points to determine a total score
that represents discrimination. We'd expect this to increase with more
clusters but alas it seems 1 cluster is locally optimal in effectively
segregating our samples. This is pretty odd, but, the numbers dont seem
to lie but the ensuing sillhouette scores aren't terribly worse, so I've
chosen to show k=5 below.

Below I visualize the sillhouette score. We can see the samples in the
two clusters that overlap, they are relatively few (the part of the red
and blue chunk below 0). The dotted red line represents the average
score, which is excellent. The X axis is individual samples though I've
removed the label to keep things clean!

```{r}
pam_result_5000_five <- pam(top_5000m, k = 5)
fviz_silhouette(pam_result_5000_five, label=FALSE)
```

This sillhouette chart is hillarious. The loss in sillhouette score
comes entirely from the red group which refuses to be separated by PAM
whilst the other groups, especially pink (5), are exceptionally
segregated with values almost exclusively in the positive domain.
Perhaps if more clusters were added, the global or at least a new local
optima K value of, say, 15 or 20 would be found, but for now monolithic
cluster 1 is dragging the score down.

Finally lets look at an alluvial. Since 10,000 for PAM is very
computationally expensive, I will instead show an alluvial at 5000 for
many K values as we already computed these above.

```{r}

top_5000G <- top_5000 %>%
  tibble::rownames_to_column("Gene")
alluvialData <- data.frame(
  K2 = pam_result_5000_two$clustering, 
  K3 = pam_result_5000_three$clustering, 
  K4 = pam_result_5000_four$clustering,
  K5 = pam_result_5000_five$clustering,
  K6 = pam_result_5000_six$clustering
)

ggplot(
  data = alluvialData, 
  aes(axis1 = K2, axis2 = K3, axis3 = K4, axis4 = K5, axis5 = K6)
) +
  geom_alluvium(aes(fill = K6)) +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme_classic() +
  xlab("Amount of Clusters") +
  ylab("Gene Index") +
  labs(title = "PAM for 5000 Samples at K=2,3,4,5,6")
```

Nasty. We can see the monolithic cluster squeezing the other groups
away, and making me ponder if a logarithimically scaled cluster size
visualization would be preferable. Alas.\

This speaks both to the significant goodness-of-cluster that this
monolith is exhibiting and to the robust cluster 2 which also undergoes
minimal subdivision in higher K values.

A lesson here is that the marginalized groups in an alluvial are often
intriguing, making it a poor visualization for those interesting
outliers (though they capture macroscopic trends that are still surely
significant and more likely to be relevant to the disease or affliction
we're usually trying to capture in bioinformatics).

3.  [**Heatmaps and Dendrograms**]{.underline}

    1.  [**Create a heatmap of the 5,000 genes used in clustering, with
        an annotation sidebar showing one set of clusters you identified
        from each clustering method and the sample groups from
        Assignment**]{.underline}
        1.  [**You should have n+1 annotation columns, 1 per clustering
            method and 1 for the sample groups\
            in Assignment 1.**]{.underline}
        2.  [**Either create a different heatmap for each method or
            include all in the same heatmap.**]{.underline}
        3.  [**Include a legend and axis labels**]{.underline}
        4.  [**Include row and column dendrograms**]{.underline}

# K-Means Heatmap

```{r}
mat_100<- data.matrix(top_100,rownames.force = FALSE)
ht = ComplexHeatmap::Heatmap(mat_100,heatmap_legend_param = list(
        title = "Expression", at = c(0, 50, 100,150)
    ),column_title = "Clustered Genes",show_column_names = FALSE,cluster_columns = TRUE,cluster_rows = TRUE)
ComplexHeatmap::draw(ht)
```

4.  [**Statistics**]{.underline}

    1.  [**Does cluster membership correlate with the groups you chose
        in Assignment 1? Perform a chi-squared test of independence to
        statistically compare the two. Do this for each clustering
        result you identified -- include all versions per clustering
        approach. You are comparing your different clustering
        results.**]{.underline}
    2.  [**Adjust all statistical test results for multiple hypothesis
        testing (p.adjust).**]{.underline}
    3.  [**Create a table with statistical test results that includes
        adjusted and un-adjusted p-values.**]{.underline}

Alright first we must filter out the metadata a little, but also
organize all the data against which we shall chi square:

```{r include=FALSE}
results <- list(
    "KMEANS_K5_N10" = kmeans_result_10_five$cluster,
    "KMEANS_K4_N10" = kmeans_result_10_four$cluster,
    "KMEANS_K3_N10" = kmeans_result_10_three$cluster,
    "KMEANS_K5_N100" = kmeans_result_100_five$cluster,
    "KMEANS_K4_N100" = kmeans_result_100_four$cluster,
    "KMEANS_K3_N100" = kmeans_result_100_three$cluster,
    "KMEANS_K5_N1000" = kmeans_result_1000_five$cluster,
    "KMEANS_K4_N1000" = kmeans_result_1000_four$cluster,
    "KMEANS_K3_N1000" = kmeans_result_1000_three$cluster,
    "KMEANS_K5_N10000" = kmeans_result_10000_five$cluster,
    "KMEANS_K4_N10000" = kmeans_result_10000_four$cluster,
    "KMEANS_K3_N10000" = kmeans_result_10000_three$cluster,
    "KMEANS_K5_N5000" = kmeans_result_5000_five$cluster,
    "KMEANS_K4_N5000" = kmeans_result_5000_four$cluster,
    "KMEANS_K3_N5000" = kmeans_result_5000_three$cluster,
    "HCLUSTER_K5_N10" = hcluster_result_10_five,
    "HCLUSTER_K4_N10" = hcluster_result_10_four,
    "HCLUSTER_K3_N10" = hcluster_result_10_three,
    "HCLUSTER_K2_N10" = hcluster_result_10_two,
    "HCLUSTER_K5_N10000" = hcluster_result_10000_five,
    "HCLUSTER_K4_N10000" = hcluster_result_10000_four,
    "HCLUSTER_K3_N10000" = hcluster_result_10000_three,
    "HCLUSTER_K2_N10000" = hcluster_result_10000_two,
    "HCLUSTER_K4_N5000" = hcluster_result_5000_four,
    "HCLUSTER_K3_N5000" = hcluster_result_5000_three,
    "HCLUSTER_K2_N5000" = hcluster_result_5000_two,
    "HCLUSTER_K1_N5000" = hcluster_result_5000_one,
    "PAM_K3_N10" = pam_result_10_three,
    "PAM_K3_N100" = pam_result_100_three,
    "PAM_K3_N1000" = pam_result_1000_three,
    "PAM_K3_N10000" = pam_result_10000_three,
    "PAM_K2_N5000" = pam_result_5000_two,
    "PAM_K3_N5000" = pam_result_5000_three,
    "PAM_K4_N5000" = pam_result_5000_four,
    "PAM_K5_N5000" = pam_result_5000_five,
    "PAM_K6_N5000" = pam_result_5000_six,
)

new_expression_df <- readr::read_tsv(data_file) %>%
  tibble::column_to_rownames("Gene")

metadata <- metadata %>%
  dplyr::mutate(diabetes = dplyr::case_when(
    stringr::str_detect(refinebio_subject, "non t2d") ~ "reference",
    stringr::str_detect(refinebio_subject, "t2d") ~ "diabetic",
  ))

#Remove ambiguously-labeled samples from metadata
culledMeta <- metadata[!(metadata$refinebio_subject=="pancreatic islets"),]

discardColumns <- metadata[(metadata$refinebio_subject=="pancreatic islets"),]
discardColumns = as.vector(discardColumns$refinebio_accession_code)
length(discardColumns)
#Preserve only columns in expression_df that match one of the accession ids
culled_expression_df = new_expression_df[,!(names(new_expression_df) %in% discardColumns)]

# Make mutation_status a factor and set the levels appropriately
culledMeta <- culledMeta %>%
  dplyr::mutate(
    # Here we define the values our factor variable can have and their order.
    diabetes = factor(diabetes, levels = c("reference", "diabetic"))
)

new_expression_df <- culledMeta %>%
  tibble::column_to_rownames("refinebio_accession_code")
```

4.  Write a short summary for each plot/table you created in this
    assignment. In 3-5 sentences for each, describe what you did, what
    parameters you used (if any) and an interesting result from it.

    âœ…

5.  As a team, fill out the team evaluation table below.

    âœ…

6.  Combine all results into a single file, submit on Canvas. Make sure
    that all your code is added to your GitHub repository.

    âœ…
